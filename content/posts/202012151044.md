---
title: "【scikit-learn練習】線性回歸"
date: 2020-12-15T10:44:10+08:00
draft: false
categories: [Machine Learning]
tags: [Python, scikit-learn]
isCJKLanguage: true
---

一樣多來些練習，這次是線性回歸(Linear Regression)
<!--more-->

做好的notebook也可直接觀看：<a target="_blank" href="https://github.com/mikanbearer/experiments/blob/master/boston_line_20201215.ipynb">傳送門</a>



### 環境
----

<br></br>
* python 3.6.8
* scikit-learn 0.23.2
* seaborn 0.10.1

<br></br>

### 載入資料集
----

<br></br>
這次練習使用load_boston()，可以從DESCR查看詳細資料，由此得知target是Median value，也就是價格中位數
```python {hl_lines=[21]}
from sklearn.datasets import load_boston


boston = load_boston()

print(boston.keys())
'''
dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])
'''
print(boston.DESCR)
'''
.. _boston_dataset:

Boston house prices dataset
---------------------------

**Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
...
'''
```
<br></br>

把Data塞進pandas的DataFrame
```python
import pandas as pd

boston_df = pd.DataFrame(data=boston.data, columns=boston.feature_names)
print(boston_df.head())
'''
      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \
0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   
1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   
2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   
3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   
4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   

   PTRATIO       B  LSTAT  
0     15.3  396.90   4.98  
1     17.8  396.90   9.14  
2     17.8  392.83   4.03  
3     18.7  394.63   2.94  
4     18.7  396.90   5.33  
'''

boston_target = pd.DataFrame(data=boston.target)
print(boston_target.head())
'''
      0
0  24.0
1  21.6
2  34.7
3  33.4
4  36.2
'''
```
<br></br>


### 繪圖
---
<br></br>

可以先稍微看一下這個dataset的樣子，使用seaborn來做可視化，接此挑個合適的值來做
```python
import seaborn as sns

%matplotlib inline

boston_df['MEDV'] = boston.target

sns.pairplot(boston_df, x_vars=['CRIM', 'NOX', 'RM', 'AGE', 'LSTAT'], y_vars=['MEDV'])

boston_df[['RM','MEDV']].corr()
```
<br></br>

直覺看起來仍是RM最高，這個RM也就是每個物件的房間數

![](1.png)
<br></br>

有將近0.7正相關

![](2.png)
<br></br>


### 分割測試集與訓練集
---
<br></br>

先來個簡單回歸熱身，簡單線性回歸就是利用單一自變數預測另一個依變數，首先是慣例的分割
```python
from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(boston_df[['RM']], boston_df['MEDV'], test_size=0.3)

print(boston_df.shape)
'''
(506,)
'''
print(x_train.shape)
'''
(354, 1)
'''

print(x_test.shape)
'''
(152, 1)
'''
```
<br></br>



### 實施訓練
---
<br></br>

就把剛才的訓練集放進來
```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(x_train, y_train)
```
<br></br>

### 評價與測試
---
<br></br>

塞進測試集，並用seaborn畫出回歸模型的圖
```python
y_pred = lr.predict(x_test)

df = pd.DataFrame(data=dict(x_test))
df['MEDV_pre'] = y_pred
df['MEDV'] = y_test

df = df.melt('RM', var_name='cols',  value_name='vals')

sns.lmplot(data=df, x='RM', y='vals', hue='cols')
```
<br></br>


![](3.png)


<br></br>

評價慘兮兮
```python
lr.score(x_test, y_test)

'''
0.5243544157385994
'''
```
<br></br>

### 多元回歸
---
<br></br>
最後來一個多元回歸，也就是多個自變數，不過評價還是慘慘
```python
x_train, x_test, y_train, y_test = train_test_split(boston_df[['CRIM', 'LSTAT']], boston_df['MEDV'], test_size=0.3)

lr = LinearRegression()
lr.fit(x_train, y_train)


lr.score(x_test, y_test)
'''
0.5561125272691634
'''
```