---
title: "scikit-learn新手入門之k-近鄰演算法"
date: 2020-12-10T15:35:42+08:00
categories: [Machine Learning]
tags: [Python, scikit-learn]
isCJKLanguage: true
---
久違的機器學習，身為資料分析&機器學習的超級新手(略)
  
這次練習的一樣是scikit-learn，主題就決定是k-近鄰演算法
<!--more-->

k-近鄰演算法(k-nearest neighbors algorithm，簡稱k-NN)是一個用於分類和回歸的無母數統計方法，這次練習的分類器就是採用該演算法

  
以下節錄自<a href="https://zh.wikipedia.org/zh-tw/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" target="_blank">維基百科</a>
```text
在k-NN分類中，輸出是一個分類族群。一個物件的分類是由其鄰居的「多數表決」確定的，
k個最近鄰居（k為正整數，通常較小）中最常見的分類決定了賦予該物件的類別。
若k = 1，則該物件的類別直接由最近的一個節點賦予。
```
<br></br>
做好的notebook也可直接觀看<a target="_blank" href="https://github.com/mikanbearer/experiments/blob/master/iris_knn_20201210.ipynb">傳送門</a>
<br></br>

### 環境
----

<br></br>
* python 3.6.8
* scikit-learn 0.23.2
* seaborn 0.10.1

<br></br>

### 載入資料集
----

<br></br>
這次練習使用load_iris()，可以從DESCR查看詳細資料
```python
from sklearn.datasets import load_iris


iris = load_iris()

print(iris.keys())
'''
dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])
'''
print(iris.DESCR)
'''
.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
    ....
'''
```
<br></br>

把Data塞進pandas的DataFrame
```python
import pandas as pd

iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
print(iris_data.head())
'''
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
0                5.1               3.5                1.4               0.2
1                4.9               3.0                1.4               0.2
2                4.7               3.2                1.3               0.2
3                4.6               3.1                1.5               0.2
4                5.0               3.6                1.4               0.2
'''


iris_target = pd.DataFrame(data=iris.target)
print(iris_target.head())
'''
        0
0  setosa
1  setosa
2  setosa
3  setosa
4  setosa
'''
```
<br></br>



### 繪製散佈圖
---
<br></br>

其實沒什麼意義，只是為了圖文並茂一下並藉此回味一下seaborn
```python
import seaborn as sns

%matplotlib inline

iris_data['target'] = iris_target

sns.pairplot(iris_data, hue='target')
```
<br></br>
看看這次資料的分佈還滿錯縱複雜的

![](1.png)

<br></br>


### 分割測試集與訓練集
---
<br></br>

```python
from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.3)

print(iris_data.shape)
'''
(150, 4)
'''
print(x_train.shape)
'''
(105, 4)
'''
print(x_test.shape)
'''
(45, 4)
'''
```
<br></br>


### 實施訓練
---
<br></br>

接下來就是前面說到的K Nearest Neighbor，分類器使用`KNeighborsClassifier()`，相關param可以參考<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank">官方文件</a>
  
這次練習的n_neighbors給1，也就是開頭敘述k-NN所指的常數k，這樣一來類別即是由最近的鄰居賦予
```python
from sklearn.neighbors import KNeighborsClassifier


knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(x_train, y_train.values.ravel())
```
<br></br>

### 評價與測試
---
<br></br>




塞進測試集
```python
from sklearn.metrics import confusion_matrix


df = pd.DataFrame(confusion_matrix(y_test,knn.predict(x_test).reshape(-1,1)))
df = df.rename(columns=dict(zip(df.columns, ['(pre) ' + name for name in iris.target_names])), index=dict(zip(df.columns, iris.target_names)))
df
```
<br></br>
出乎意料沒有預測錯誤的？

![](2.png)
<br></br>

評價…不知怎麼這次有點準
```python
knn.score(x_test, y_test)

'''
1.0
'''
```