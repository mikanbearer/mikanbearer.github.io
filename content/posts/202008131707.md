---
title: "使用Prometheus監控RabbitMQ之從頭動手"
date: 2020-08-13T17:07:15+08:00
draft: true
categories: [監控工具]
tags: [Prometheus, RabbitMQ]
---
Prometheus是最近非常流行的Pull-based monitoring，主要的角色是以下兩者：
  
* Prometheus：監控方，定期收集資料並存在資料庫
* Exporter：被監控方，回傳監控方需要的資料
  
為了往後學習方便，先來實作一遍這個方便利用的工具

<!--more-->
Prometheus的基礎運用架構很單純，大在就像下圖：
![](0.png)
  
從圖中可見，Prometheus收集時間序列資料(metrics)都是由自己主動向Exporter發起的，這次大概會做這些：
  

* <a href="#1">啟動Prometheus</a>
* <a href="#2">啟動Exporter</a>
* <a href="#3">使用static config監控</a>
* <a href="#4" >使用file-based service discovery監控</a>
* <a href="#5">監控RabbitMQ</a>
  

<br></br>



* <h3 id=1>啟動Prometheus</h3>
<br></br>
部屬Prometheus非常簡單，就只是binary配上一個設定檔，指令如下：
```
$ wget https://github.com/prometheus/prometheus/releases/download/v2.20.1/prometheus-2.20.1.linux-amd64.tar.gz
$ tzr zxvf prometheus-2.20.1.linux-amd64.tar.gz
$ cd tzr zxvf prometheus-2.20.1.linux-amd64
```
<br></br>
因為預設的prometheus.yml已經定義好自己了，所以馬上就能用了
```
$ ./prometheus --config.file=prometheus.yml
```
<br></br>
使用cURL測試一下metrics
```
$ curl localhost:9090/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 8.412e-06
go_gc_duration_seconds{quantile="0.25"} 6.8151e-05
go_gc_duration_seconds{quantile="0.5"} 0.000435588
go_gc_duration_seconds{quantile="0.75"} 0.000661594
go_gc_duration_seconds{quantile="1"} 0.001019489
go_gc_duration_seconds_sum 0.002193234
go_gc_duration_seconds_count 5
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
...
```
**就算加上下載，整個過程也不到五分鐘…**
  
拿到metrics就是這麼快速，也能進web看看graph，在瀏覽器打上http://prometheus_ip:9090/graph就能看到了，
tab選擇graph也能看到些簡單的圖表
![](1.png)
<br></br>

* <h3 id="2">啟動Exporter</h3>
<br></br>
既然Prometheus已經起來了，那麼來試著監控些什麼，就下載個node exporter來測試
```
wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz
tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz
cd node_exporter-1.0.1.linux-amd64
```
<br></br>
裡面一樣是個binary file，執行下去
```
 ./node_exporter --web.listen-address 127.0.0.1:8080
curl 127.0.0.1:8080/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
```
<br></br>
cURL測試metrics
```
$ curl localhost:8080/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 9.31e-06
go_gc_duration_seconds{quantile="0.25"} 5.488e-05
go_gc_duration_seconds{quantile="0.5"} 0.000112915
go_gc_duration_seconds{quantile="0.75"} 0.000194093
go_gc_duration_seconds{quantile="1"} 0.004481425
go_gc_duration_seconds_sum 0.175059981
go_gc_duration_seconds_count 957
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 8
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
...
```
一樣簡單快速
<br></br>

* <h3 id="3">使用static config進行監控</h3>
<br></br>
既然exporter沒問題，那麼是時候建立一個job來get metrics了，就依照[這裡的範例](https://prometheus.io/docs/prometheus/latest/getting_started/)，一步步操作吧
  
依照範例，建立prometheus.rules.yml，用於依照需求控制紀錄的方式，能降低監控造成的負擔，此範例是記錄下五分鐘平均的cpu狀態
```:prometheus.rules.yml {linenos=table, linenostart=1}
groups:
- name: cpu-node
  rules:
  - record: job_instance_mode:node_cpu_seconds:avg_rate5m
    expr: avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))
```
<br></br>
然後修改prometheus.yml，除了增加rule，也新增job，並[之前]()啟動的exporter
```:prometheus.yml {linenos=table, linenostart=1, hl_lines=[10, "21-33"]}
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.
  evaluation_interval: 15s # Evaluate rules every 15 seconds.

  # Attach these extra labels to all timeseries collected by this Prometheus instance.
  external_labels:
    monitor: 'codelab-monitor'

rule_files:
  - 'prometheus.rules.yml'

scrape_configs:
  - job_name: 'prometheus'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: ['localhost:9090']

  - job_name:       'node'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: ['localhost:8080', 'localhost:8081']
        labels:
          group: 'production'

      - targets: ['localhost:8082']
        labels:
          group: 'canary'
```
<br></br>
重新啟動prometheus後，就能用去query job instance了
![](2.png)
<br></br>
  
* <h3 id="4">使用file-based service discovery監控</h3>
<br></br>
在實務上，不可能每增加個exporter就重新啟動一次，這時候file-based service discovery就很重要，
這種方式和static config不同，只要修改檔案讓prometheus能去讀取就行，不需要reload
  
先在目錄中建立一個空的file_sd.yml，並修改prometheus.yml
```:prometheus.yml {linenos=table, linenostart=21, hl_lines=[8]}
  - job_name:       'node'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    file_sd_configs:
      - files:
        - ./file_sd.yml
```
<br></br>

重新啟動prometheus後，就發現targets回到空空如也的狀態了
![](3.png)
<br></br>

接下來就是要用file來增加targets了，把前版的prometheus.yml中的static config放到file_sd.yml
```:file_sd.yml {linenos=table, linenostart=1}
- targets: ['localhost:8080', 'localhost:8081']
  labels:
    group: 'production'

- targets: ['localhost:8082']
  labels:
    group: 'canary'
```
<br></br>

接下來不需要重新啟動prometheus，也能新增targets了
![](4.png)

* <h3 id=5>監控RabbitMQ</h3>
<br></br>
因為常常使用RabbitMQ，要是能監控服務的話還真是幫了大忙，
  
而RabbitMQ 3.8.X內建了Prometheus的plugin，這次就來測試看看，當作實務運用的練習
  
[rabbitmq-prometheus的repository](https://github.com/rabbitmq/rabbitmq-prometheus)
  
一樣求方便使用docker-compose，寫個陽春的Dockerfile會比較方便
  
為了使用內建的pulgin，直接用3.8.0以後的image，並且RUN啟用的command
```Dockerfile
FROM rabbitmq:3.8.6-management
RUN rabbitmq-plugins enable rabbitmq_prometheus
```
<br></br>
compose file增加port 15692
```:docker-compose.yml {linenos=table, linenostart=1, hl_lines=[6, 10]}
version: '3'

services:
  my-queue:
    container_name: rabbitmq
    build: ./
    ports:
      - '5672:5672'
      - '15672:15672'
      - '15692:15692'
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq

volumes:
    rabbitmq-data:
```
<br></br>
接下來就能在host上用cURL測試了
```
$ curl -v -H "Accept:text/plain" "http://localhost:15692/metrics"
...
# TYPE rabbitmq_consumers gauge
# HELP rabbitmq_consumers Consumers currently connected
rabbitmq_consumers 0
# TYPE rabbitmq_queues gauge
# HELP rabbitmq_queues Queues available
rabbitmq_queues 0
# TYPE rabbitmq_build_info untyped
# HELP rabbitmq_build_info RabbitMQ & Erlang/OTP version info
rabbitmq_build_info{rabbitmq_version="3.8.6",prometheus_plugin_version="3.8.6",prometheus_client_version="4.6.0",erlang_version="23.0.3"} 1
# TYPE rabbitmq_identity_info untyped
# HELP rabbitmq_identity_info RabbitMQ node & cluster identity info
rabbitmq_identity_info{rabbitmq_node="rabbit@7f33446fce98",rabbitmq_cluster="rabbit@7f33446fce98"} 1
# TYPE telemetry_scrape_duration_seconds summary
# HELP telemetry_scrape_duration_seconds Scrape duration
# TYPE telemetry_scrape_size_bytes summary
# HELP telemetry_scrape_size_bytes Scrape size, not encoded
# TYPE telemetry_scrape_encoded_size_bytes summary
...
```
巴拉巴拉一大堆
<br></br>
適當增加一下prometheus的config，反正只是測試一下，就姑且用static config吧
```yaml
scrape_configs:
...
  - job_name: 'rabbitmq-exporter'
    scrape_interval: 60s
    scrape_timeout: 59s
    static_configs:
      - targets:
          - 'rabbitmq-exporter:15692'
```
<br></br>
能看到endpoint增加了
![](1.png)
<br></br>
![](2.png)
這樣就能用Prometheus監控RabbitMQ了
<br></br>