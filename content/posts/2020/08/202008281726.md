---
title: "從Scrapy開始玩爬蟲"
date: 2020-08-28T17:26:08+08:00
draft: false
categories: [資料分析]
tags: [Python, Scrapy]
isCJKLanguage: true
---
為了之後的學習方向做準備，來練習一下Scrapy，Scrapy就是處理response的工具，並從中取得自己需要的資料，
  
這次就做個最低限度的練習
<!--more-->
這次主要接觸的就以下三者：
  
* Spider
  * 用於爬網站
* Item
  * 將爬到的東西構造話的資料模型
* Pipeline
  * 處理Spider回傳的Item
<br></br>
  
開始動手囉
<br></br>
### 事前準備

---
<br></br>
首先安裝必要的包
```
pip install Scrapy
```
<br></br>
接下來就可以用指令建立project
```
$ scrapy startproject scrapy_toturial
```
<br></br>
project目錄下的樹狀圖是這樣的
```
scrapy_tutorial
│  scrapy.cfg
│
└─scrapy_tutorial
    │  items.py
    │  middlewares.py
    │  pipelines.py
    │  settings.py
    │  __init__.py
    │
    └─spiders
            __init__.py
```
接下來就可以開始動手做啦
<br></br>
### 建立一個Spider

---
<br></br>
接下來就是建立spider了，spider就是負責取得response的關鍵，範例就這個很久沒更新的[hugo theme ranking](https://hugo-theme-ranking.oika.me/)吧！
  
輸入以下指令就會在當前目錄建立spider，為求整齊，建立在/spiders底下
```
$ scrapy genspider hugo_ranking https://hugo-theme-ranking.oika.me/
```
<br></br>
現在的樹狀圖會是這樣
```
scrapy_tutorial
│  scrapy.cfg
│
└─scrapy_tutorial
    │  items.py
    │  middlewares.py
    │  pipelines.py
    │  settings.py
    │  __init__.py
    │
    └─spiders
            hugo_ranking.py ★
            __init__.py

```
<br></br>
預設的spider是這個樣子，一切的關鍵就在於這個parse()
```:scrapy_toturial/spiders/hugo_ranking.py {linenos=table, linenostart=1}
import scrapy


class HugoRankingSpider(scrapy.Spider):
    name = 'hugo_ranking'
    allowed_domains = ['hugo-theme-ranking.oika.me']
    start_urls = ['http://hugo-theme-ranking.oika.me/']

    def parse(self, response):
        pass
```
<br></br>
也可以用指令列出當前project的spiders
```
$ scrapy list
hugo_ranking
```
<br></br>
接下來就可以「爬」一遍啦
```
$ scrapy crawl hugo_ranking
...
 'start_time': datetime.datetime(2020, 8, 31, 5, 56, 48, 766086)}
2020-08-31 13:56:51 [scrapy.core.engine] INFO: Spider closed (finished)

```
當然除了log外是沒輸出的，因為parse()的內容還沒定義
<br></br>
主要需要coding的是這個地方，像以下的範例，會將html以text的格式print出來，也就是stdout
```:scrapy_toturial/spiders/hugo_ranking.py {linenos=table, linenostart=1, hl_lines=[10]}
import scrapy


class HugoRankingSpider(scrapy.Spider):
    name = 'hugo_ranking'
    allowed_domains = ['hugo-theme-ranking.oika.me']
    start_urls = ['http://hugo-theme-ranking.oika.me/']

    def parse(self, response):
        print(response.text)
```
<br></br>
當然也能用open把html存成檔案
```:scrapy_toturial/spiders/hugo_ranking.py {linenos=table, linenostart=1, hl_lines=["10-12"]}
import scrapy


class HugoRankingSpider(scrapy.Spider):
    name = 'hugo_ranking'
    allowed_domains = ['hugo-theme-ranking.oika.me']
    start_urls = ['http://hugo-theme-ranking.oika.me/']

    def parse(self, response):
        filename = '%s.html' % response.url.split('/')[-2]
        f = open(filename, 'wb')
        f.write(response.body)
```
<br></br>
可以開始試著抓資料囉，接著就是用selector來取得自己想要的部分，這次的範例就先參考list
```html
...

						<ul class="theme-list">
							<li class="theme-item">
								<div class="theme-border">
									<a href="https://themes.gohugo.io/academic/" target="_blank" rel="noopener"><img src="images\thums\Academic.png"/></a>
									<span>
										<span class="theme-label">1. <a href="https://themes.gohugo.io/academic/" target="_blank">Academic</a> (<a href="https://github.com/gcushen/hugo-academic.git">3065</a>)</span>
										<span class="tags">#academic #portfolio #responsive #student #personal #university #blog #minimal</span>
									</span>
								</div>
							</li>
...
```
<br></br>
下面的範例就是用yield，讓parse()成為一個generator，回傳的值會是dict
```:scrapy_toturial/spiders/hugo_ranking.py {linenos=table, linenostart=1, hl_lines=["11-16"]}
import scrapy

class HugoRankingSpider(scrapy.Spider):
    name = 'hugo_ranking'
    allowed_domains = ['hugo-theme-ranking.oika.me']
    start_urls = ['http://hugo-theme-ranking.oika.me/']

    def parse(self, response):

        for theme in response.css('li.theme-item'):
            yield {
                'neme': theme.css('span.theme-label a::text').get(),
                'url':  theme.css('span.theme-label a::attr(href)').get(),
                'star': theme.css('span.theme-label a:nth-child(2)::text').get(),
                'tags': theme.css('span.tags::text').get(),
            }
```
<br></br>
輸出就會像這樣子，看得出有抓到想要的東西了
```
2020-08-31 15:50:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hugo-theme-ranking.oika.me/>
{'neme': 'Academic', 'url': 'https://themes.gohugo.io/academic/', 'star': '3065', 'tags': ['#academic', '#portfolio', '#responsive', '#student', '#personal', '#university', '#blog', '#minimal']}
2020-08-31 15:50:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hugo-theme-ranking.oika.me/>
{'neme': 'Learn', 'url': 'https://themes.gohugo.io/hugo-theme-learn/', 'star': '692', 'tags': ['#documentation', '#grav', '#learn', '#doc', '#search']}
2020-08-31 15:50:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hugo-theme-ranking.oika.me/>

```
<br></br>
### 使用Items

---
<br></br>
接下來加入Item的部分，其實這東西就是一個model，裡面就定義好自己所需的field
```:scrapy_toturial/items.py {linenos=table, linenostart=1, hl_lines=["12-16"]}
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class ScrapyTutorialItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()
    url = scrapy.Field()
    star = scrapy.Field()
    tags = scrapy.Field()
```
<br></br>
將剛才定義的item導入spider，一樣用generator
```:scrapy_toturial/spiders/hugo_ranking.py {linenos=table, linenostart=1, hl_lines=["10-21", 2]}
import scrapy
from ..items import ScrapyTutorialItem

class HugoRankingSpider(scrapy.Spider):
    name = 'hugo_ranking'
    allowed_domains = ['hugo-theme-ranking.oika.me']
    start_urls = ['http://hugo-theme-ranking.oika.me/']

    def parse(self, response):
        for theme in response.css('li.theme-item'):
            item = ScrapyTutorialItem()

            item['name'] = theme.css('span.theme-label a::text').get()
            item['url'] = theme.css('span.theme-label a::attr(href)').get()
            item['star'] = theme.css('span.theme-label a:nth-child(2)::text').get()
            item['tags'] = theme.css('span.tags::text').get()

            yield item
```
<br></br>
然後使用指令就能輸出json了
```
$ scrapy crawl hugo_ranking -o data.json
```
<br></br>
可以驗收一下成果
```json
[
{"name": "Academic", "url": "https://themes.gohugo.io/academic/", "star": "3065", "tags": "#academic #portfolio #responsive #student #personal #university #blog #minimal"},
{"name": "Learn", "url": "https://themes.gohugo.io/hugo-theme-learn/", "star": "692", "tags": "#documentation #grav #learn #doc #search"},
{"name": "Even", "url": "https://themes.gohugo.io/hugo-theme-even/", "star": "654", "tags": "#responsive #blog #simple #clean #highlight.js #syntax highlighting"},
{"name": "Tranquilpeak", "url": "https://themes.gohugo.io/hugo-tranquilpeak-theme/", "star": "516", "tags": "#blog #minimal #responsive #personal #customizable #creative #fontawesome #highlight.js"},

...

]
```
<br></br>
### 使用Pipeline

---
<br></br>
使用Item其實也就是方便整理資料，為了將爬出來的東西複用，接下來就能用pipeline來處理這個item該做些什麼，首先修改setting
  
當初建立project的時候這行就自動產生了，只要取消註解便可
<br></br>
```:scrapy_toturial/settings.py {linenos=table, linenostart=65, hl_lines=["1-3"]}
ITEM_PIPELINES = {
    'scrapy_tutorial.pipelines.ScrapyTutorialPipeline': 300,
}
```
<br></br>
接下來就是定義pipeline了，當初我們使用generator就是為此，這樣parse()執行一次，就會將item交由process_item()處理，像以下範例就能存到sqlite3內
```:scrapy_toturial/pipelines.py {linenos=table, linenostart=1}
from itemadapter import ItemAdapter
import sqlite3


class ScrapyTutorialPipeline:
    conn = None

    @classmethod
    def get_database(cls):
        cls.conn = sqlite3.connect('items.db')

        cursor = cls.conn.cursor()
        cursor.execute(
            'CREATE TABLE IF NOT EXISTS theme(\
                id INTEGER PRIMARY KEY AUTOINCREMENT, \
                name TEXT NOT NULL, \
                url TEXT, \
                star INTEGER, \
                tags TEXT\
            );')

        return cls.conn

    def process_item(self, item, spider):
        self.save_item(item)
        return item

    def save_item(self, item):
        db = self.get_database()
        db.execute(
            'INSERT INTO theme(name, url, star, tags) VALUES (?, ?, ?, ?)',
            tuple(ItemAdapter(item).values())
        )
        db.commit()
```
<br></br>
然後就能打開shell驗收了
```
>>> import sqlite3
>>> db = sqlite3.connect('scrapy_tutorial/scrapy_tutorial/spiders/items.db')
>>> record = db.execute('select * from theme')
>>> record.fetchall()
[(1, 'Academic', 'https://themes.gohugo.io/academic/', 3065, '#academic #portfolio #responsive #stu
```
練習完收工
