---
title: "使用cephadm部屬Ceph Cluster"
date: 2020-11-10T16:52:54+08:00
draft: false
categories: [Deployment]
tags: [Ceph]
isCJKLanguage: true
---
cephadm是官方推薦的安裝方式之一(另一個是rook)，先來試做看看順不順手，目標是建立一個可掛載使用的Cluster

<!--more-->
準備三個guest os，這次環境是CentOS 7
```
# cat /etc/*-release
CentOS Linux release 7.7.1908 (Core)
NAME="CentOS Linux"
VERSION="7 (Core)"
...
```
<br></br>

預計做成以下形式
|||
-|-|-
node1|192.168.149.131|mgr, mon, osd, mds
node2|192.168.149.132|mon, osd, mds
node3|192.168.149.133|mon, osd, mds

<br></br>
主要步驟如下

* <a onclick="window.scrollTo({top: document.getElementById(1).offsetTop, behavior: 'smooth'})">準備測試環境</a>
* <a onclick="window.scrollTo({top: document.getElementById(2).offsetTop, behavior: 'smooth'})">建立Cluster</a>
* <a onclick="window.scrollTo({top: document.getElementById(3).offsetTop, behavior: 'smooth'})">部屬OSD與MDS</a>
* <a onclick="window.scrollTo({top: document.getElementById(4).offsetTop, behavior: 'smooth'})">建立CephFS並掛載</a>


<br></br>


<h3 id=1>準備測試環境</h3>

---
<br></br>


首先安裝cephadm執行時需要的python3與校時的chrony
```
# yum install -y python3 chrony
```
<br></br>
安裝完啟用chrony
```
# systemctl start chronyd
# systemctl enable chronyd
```
<br></br>
Ceph的daemon是以container形式存在於host上，所以需要docker或是podman，否則會出現這樣的錯誤
```
ERROR: Unable to locate any of ['podman', 'docker']
```
<br></br>
這裡的範例是安裝docker(需增加repo)
```
# yum install -y yum-utils
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# yum install -y docker-ce docker-ce-cli containerd.io
```
<br></br>
安裝完記得啟動
```
# systemctl start docker
# systemctl enable docker
```
<br></br>

增加hosts
```
[root@node1 ~]# cat >> /etc/hosts <<EOF
> 192.168.149.131 node1
> 192.168.149.132 node2
> 192.168.149.133 node3
> EOF
```
<br></br>



<h3 id=2>建立Cluster</h3>

---
<br></br>

以下動作全都在node1執行，接下來就是下載cephadm並增加執行權限，cephadm可以直接使用不需要安裝，所以這裡就不安裝了
```
[root@node1 ~]# curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
[root@node1 ~]# chmod +x cephadm 
```
<br></br>


接下使用cephadm建立cluster，node1將作為mgr與mon
```
[root@node1 ~]# mkdir -p /etc/ceph
[root@node1 ~]# ./cephadm bootstrap --mon-ip 192.168.149.131
...
Ceph Dashboard is now available at:

             URL: https://node1:8443/
            User: admin
        Password: krbpvjv6t3

You can access the Ceph CLI with:

        sudo ./cephadm shell --fsid e36ef05a-287a-11eb-a943-000c2929c13d -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

```

<br></br>

完成會有個不錯漂亮的grafana dashboard
![](1.png)
<br></br>
![](2.png)
<br></br>

可以看看cephadm部屬的container
```
[root@node1 ~]# docker ps
CONTAINER ID        IMAGE                       COMMAND                  CREATED              STATUS              PORTS               NAMES
33e2256e001d        ceph/ceph:v15               "/usr/bin/ceph-crash…"   29 seconds ago       Up 28 seconds                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node1
0e415c9753b5        prom/alertmanager:v0.20.0   "/bin/alertmanager -…"   33 seconds ago       Up 32 seconds                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-alertmanager.node1
e1c850700429        ceph/ceph:v15               "/usr/bin/ceph-mgr -…"   About a minute ago   Up About a minute                       ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mgr.node1.vhujdv
4a8881027cb6        ceph/ceph:v15               "/usr/bin/ceph-mon -…"   About a minute ago   Up About a minute                       ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node1
```
<br></br>


可以使用cephadm shell來使用ceph指令，並以此來操作cluster
```
[root@node1 ~]# ./cephadm shell
Inferring fsid e36ef05a-287a-11eb-a943-000c2929c13d
Inferring config /var/lib/ceph/e36ef05a-287a-11eb-a943-000c2929c13d/mon.node1/config
Using recent ceph image ceph/ceph:v15
[ceph: root@node1 /]# 
```
<br></br>

查看cluster狀態
```txt {hl_lines=["8-11"]}
[ceph: root@node1 /]# ceph status
  cluster:
    id:     e36ef05a-287a-11eb-a943-000c2929c13d
    health: HEALTH_WARN
            Reduced data availability: 1 pg inactive
            OSD count 0 < osd_pool_default_size 3
 
  services:
    mon: 1 daemons, quorum node1 (age 3m)
    mgr: node1.vhujdv(active, since 2m)
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             1 unknown
 
[ceph: root@node1 /]# ceph orch ps
NAME                 HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                  IMAGE ID      CONTAINER ID  
alertmanager.node1   node1  running (19s)  12s ago    2m   0.20.0   prom/alertmanager:v0.20.0   0881eb8f169f  3eda8ed9c0a4  
crash.node1          node1  running (2m)   12s ago    2m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  33e2256e001d  
grafana.node1        node1  running (16s)  12s ago    63s  6.6.2    ceph/ceph-grafana:6.6.2     a0dce381714a  bbda408ab66a  
mgr.node1.vhujdv     node1  running (3m)   12s ago    3m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  e1c850700429  
mon.node1            node1  running (3m)   12s ago    3m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  4a8881027cb6  
node-exporter.node1  node1  running (49s)  12s ago    59s  0.18.1   prom/node-exporter:v0.18.1  e5a616e4b9cf  63daaba5a0a3  
prometheus.node1     node1  running (33s)  12s ago    33s  2.18.1   prom/prometheus:v2.18.1     de242295e225  0c0ea49443a2   
```

<br></br>
接下來加入新的Host，為了無密碼登入，將cephadm產生的public key複製進node2與node3
```
[root@node1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2
[root@node1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3
```
<br></br>

加入node2及node3
```
[ceph: root@node1 /]# ceph orch host add node2
Added host 'node2'
[ceph: root@node1 /]# ceph orch host add node3
Added host 'node3'
```
<br></br>

可以看到host增加了
```
[ceph: root@node1 /]# ceph orch host ls       
HOST   ADDR   LABELS  STATUS  
node1  node1                  
node2  node2                  
node3  node3    
```
<br></br>

cephadm會自動在新增加的host上增加mon，並部屬相應的container
```
[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
10ff38cdf622        prom/node-exporter:v0.18.1   "/bin/node_exporter …"   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-node-exporter.node2
1207bf201a7d        ceph/ceph:v15                "/usr/bin/ceph-mon -…"   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node2
3fa73ca5bf9c        ceph/ceph:v15                "/usr/bin/ceph-crash…"   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node2
```
<br></br>

回到cephadm shell，可以看到cluster status中的mon增加了，且會自動產生一個standby的mgr
```txt {hl_lines=[9]}
[ceph: root@node1 /]# ceph status
  cluster:
    id:     e36ef05a-287a-11eb-a943-000c2929c13d
    health: HEALTH_WARN
            Reduced data availability: 1 pg inactive
            OSD count 0 < osd_pool_default_size 3
 
  services:
    mon: 3 daemons, quorum node1,node2,node3 (age 17m)
    mgr: node1.vhujdv(active, since 46m), standbys: node3.djmubq
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             1 unknown
```

<br></br>



<h3 id=3>部屬OSD與MDS</h3>

---
<br></br>
接下來部屬osd，可以用以下指令列出所有host上的device，只有乾淨的device可以部屬osd
```
[ceph: root@node1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS                                          
node1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           
node1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (<5GB) on vgs, LVM detected  
node2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           
node2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, locked, Insufficient space (<5GB) on vgs  
node3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           
node3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, LVM detected, Insufficient space (<5GB) on vgs     
```
<br></br>
可用以下指令部屬osd
```
# ceph orch daemon add osd *<host>*:*<device-path>*
```
<br></br>

但為了方便就apply all，如此一來就會為所有空的device部屬osd
```
[ceph: root@node1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
```
<br></br>

會在host上部屬對應的container，每個osd都會有一個container
```txt {hl_lines=[4]}
[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
5fb62081081a        ceph/ceph:v15                "/usr/sbin/ceph-volu…"   1 second ago        Up 1 second                             gallant_hellman
dc8997dca2e2        ceph/ceph:v15                "/usr/bin/ceph-osd -…"   8 seconds ago       Up 7 seconds                            ceph-e36ef05a-287a-11eb-a943-000c2929c13d-osd.0
10ff38cdf622        prom/node-exporter:v0.18.1   "/bin/node_exporter …"   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-node-exporter.node2
1207bf201a7d        ceph/ceph:v15                "/usr/bin/ceph-mon -…"   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node2
3fa73ca5bf9c        ceph/ceph:v15                "/usr/bin/ceph-crash…"   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node2

```
<br></br>


回到node1上也能看到osd增加了
```txt {hl_lines=[9]}
[ceph: root@node1 /]# ceph status
  cluster:
    id:     e36ef05a-287a-11eb-a943-000c2929c13d
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum node1,node2,node3 (age 19m)
    mgr: node1.vhujdv(active, since 48m), standbys: node3.djmubq
    osd: 3 osds: 3 up (since 33s), 3 in (since 33s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 57 GiB / 60 GiB avail
    pgs:     1 active+clean
```
<br></br>

部屬成功的話available會變成false
```
[ceph: root@node1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS                                          
node1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (<5GB) on vgs, locked  
node1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (<5GB) on vgs, locked  
node2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (<5GB) on vgs, locked  
node2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (<5GB) on vgs, locked  
node3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (<5GB) on vgs, LVM detected  
node3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (<5GB) on vgs, LVM detected  
```
<br></br>
可透過以下指令查看osd的狀態
```
[ceph: root@node1 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         0.05846  root default                             
-7         0.01949      host node1                           
 2    hdd  0.01949          osd.2       up   1.00000  1.00000
-3         0.01949      host node2                           
 0    hdd  0.01949          osd.0       up   1.00000  1.00000
-5         0.01949      host node3                           
 1    hdd  0.01949          osd.1       up   1.00000  1.00000
```
<br></br>

接下來部屬mds使用label來部屬，首先三台node都加上mds的label
```
[ceph: root@node1 /]# ceph orch host label add node1 mds
Added label mds to host node1
[ceph: root@node1 /]# ceph orch host label add node2 mds
Added label mds to host node2
[ceph: root@node1 /]# ceph orch host label add node3 mds
Added label mds to host node3
```
<br></br>
host的label會變成這種狀態
```
[ceph: root@node1 /]# ceph orch host ls
HOST   ADDR   LABELS  STATUS  
node1  node1  mds             
node2  node2  mds             
node3  node3  mds  
```
<br></br>
apply給他打下去
```
[ceph: root@node1 /]# ceph orch apply mds cephfs label:mds
Scheduled mds.cephfs update...
```
<br></br>
如此一來會多三個standby的mds
```txt {hl_lines=[9]}
[ceph: root@node1 /]# ceph status
  cluster:
    id:     e36ef05a-287a-11eb-a943-000c2929c13d
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum node1,node2,node3 (age 77m)
    mgr: node1.vhujdv(active, since 106m), standbys: node3.djmubq
    mds:  3 up:standby
    osd: 3 osds: 3 up (since 58m), 3 in (since 58m)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 57 GiB / 60 GiB avail
    pgs:     1 active+clean
```
<br></br>
也能用以下指令看狀態
```
[ceph: root@node1 /]# ceph fs status
    STANDBY MDS      
cephfs.node3.vpooup   
cephfs.node2.dakinl  
cephfs.node1.rvifdz
MDS version: ceph version 15.2.5 (2c93eff00150f0cc5f106a559557a58d3d7b6f1f) octopus (stable)
```
<br></br>

<h3 id=4>建立CephFS並掛載</h3>

---
<br></br>
接下來就讓Ceph自動建立一個名為cephfs的CephFS
```
[ceph: root@node1 /]# ceph fs volume create cephfs
```
<br></br>
建立完成後就變成一個active，兩個standby，做到這種狀態就能掛載使用了
```
[ceph: root@node1 /]# ceph fs status
cephfs - 0 clients
======
RANK  STATE           MDS             ACTIVITY     DNS    INOS  
 0    active  cephfs.node1.rvifdz  Reqs:    0 /s    10     13   
       POOL           TYPE     USED  AVAIL  
cephfs.cephfs.meta  metadata  1536k  17.9G  
cephfs.cephfs.data    data       0   17.9G  
    STANDBY MDS      
cephfs.node2.dakinl  
cephfs.node3.vpooup  
MDS version: ceph version 15.2.5 (2c93eff00150f0cc5f106a559557a58d3d7b6f1f) octopus (stable)
```
<br></br>
最後就是掛載了，client需要的key在cephadm的host內找
```
[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
        key = AQDUMrNf+GymGhAAcqXCfoQjf1A+KXq+dqIPLQ==

```
<br></br>
mount指令如下，ip位置為monitor的ip
```
[root@node2 ~]# mkdir -p /mnt/ceph
[root@node2 ~]# mount -t ceph 192.168.149.131:/ /mnt/cephfs -o name=admin,secret=AQDUMrNf+GymGhAAcqXCfoQjf1A+KXq+dqIPLQ==
```

