---
title: "使用cephadm部屬Ceph Cluster"
date: 2020-11-10T16:52:54+08:00
draft: false
categories: [Deployment]
tags: [Ceph]
isCJKLanguage: true
---
cephadm是官方推薦的安裝方式之一(另一個是rook)，先來試做看看順不順手

※
<br></br>


<!--more-->
os使用的是Debian 10.0.6，預計做成下列這樣

|||
-|-|-
node1|192.168.149.131|mgr, mon, osd
node2|192.168.149.132|mon, osd
node3|192.168.149.133|mon, osd

<br></br>
主要步驟如下

* <a onclick="window.scrollTo({top: document.getElementById(1).offsetTop, behavior: 'smooth'})">準備測試環境</a>
* <a onclick="window.scrollTo({top: document.getElementById(2).offsetTop, behavior: 'smooth'})">建立cluster</a>
* <a onclick="window.scrollTo({top: document.getElementById(3).offsetTop, behavior: 'smooth'})">加入新的host</a>
* <a onclick="window.scrollTo({top: document.getElementById(4).offsetTop, behavior: 'smooth'})">加入新的osd</a>
<br></br>



<h3 id=1>準備測試環境</h3>

---
<br></br>

cephadm部屬的ceph組件都運行於container中，所以需要docker或是podman，而osd也依賴lvm，所以必須安裝上述的包，否則會出現這樣的錯誤
```
ERROR: Unable to locate any of ['podman', 'docker']
lvcreate binary does not appear to be installed
```
<br></br>

這裡的範例是安裝docker
```
# apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common
# curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -
# add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"
# apt update
# apt install docker-ce docker-ce-cli containerd.io
```
<br></br>
然後是安裝lvm2
```
# apt install lvm2
```
<br></br>
lvcreate等binary預設是放在/usr/sbin，如果$PATH沒有包含此目錄的話，cephadm會找不到，所以加進$PATH
```
# export PATH="$usr/sbin:$PATH"
```
<br></br>
因為os用的是debian，sshd預設是關閉root登入的，為了方便cephadm自動部屬所以把root login打開
```:/etc/ssh/sshd_config
...
#PermitRootLogin prohibit-password
PermitRootLogin yes
```
<br></br>

增加hosts
```
root@node1:/# cat >> /etc/hosts <<EOF
> 192.168.149.131 node1
> 192.168.149.132 node2
> 192.168.149.133 node3
> EOF
```
<br></br>



<h3 id=2>建立cluster</h3>

---
<br></br>

以下動作全都在node1執行，接下來就是下載cephadm並增加執行權限，cephadm可以直接使用不需要安裝，所以這裡就不安裝了
```
root@node1:~# curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
root@node1:~# chmod +x cephadm 
```
<br></br>


接下使用cephadm建立cluster，node1將作為mgr與mon
```
root@node1:~# mkdir -p /etc/ceph
root@node1:~# ./cephadm bootstrap --mon-ip 192.168.149.131
...
Ceph Dashboard is now available at:

             URL: https://node1:8443/
            User: admin
        Password: hcjgpusdlv

You can access the Ceph CLI with:

        sudo ./cephadm shell --fsid 47f144d8-27bd-11eb-82c3-000c291ffa17 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

```

<br></br>

完成會有個不錯漂亮的grafana dashboard
![](1.png)
<br></br>
![](2.png)
<br></br>

可以看看cephadm部屬的container
```
root@node1:~# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b6284acbe6f3        prom/alertmanager:v0.20.0    "/bin/alertmanager -…"   21 seconds ago      Up 20 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-alertmanager.node1
5261bde49c13        ceph/ceph-grafana:6.6.2      "/bin/sh -c 'grafana…"   23 seconds ago      Up 22 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-grafana.node1
891b0844db44        prom/prometheus:v2.18.1      "/bin/prometheus --c…"   29 seconds ago      Up 28 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-prometheus.node1
3c010514e9cc        prom/node-exporter:v0.18.1   "/bin/node_exporter …"   45 seconds ago      Up 44 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-node-exporter.node1
4fc0d53fd204        ceph/ceph:v15                "/usr/bin/ceph-crash…"   2 minutes ago       Up 2 minutes                            ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-crash.node1
0537b9df6bbd        ceph/ceph:v15                "/usr/bin/ceph-mgr -…"   3 minutes ago       Up 3 minutes                            ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-mgr.node1.dsbbba
e749dae73c82        ceph/ceph:v15                "/usr/bin/ceph-mon -…"   3 minutes ago       Up 3 minutes                            ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-mon.node1
```
<br></br>


可以使用cephadm shell來使用ceph指令，並以此來操作cluster
```
root@node1:~# ./cephadm shell
Inferring fsid 47f144d8-27bd-11eb-82c3-000c291ffa17
Inferring config /var/lib/ceph/47f144d8-27bd-11eb-82c3-000c291ffa17/mon.node1/config
Using recent ceph image ceph/ceph:v15
```
<br></br>

查看cluster狀態
```txt {hl_lines=["8-11"]}
root@node1:/# ceph status
  cluster:
    id:     47f144d8-27bd-11eb-82c3-000c291ffa17
    health: HEALTH_WARN
            Reduced data availability: 1 pg inactive
            OSD count 0 < osd_pool_default_size 3
 
  services:
    mon: 1 daemons, quorum node1 (age 5m)
    mgr: node1.dsbbba(active, since 4m)
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             1 unknown

root@node1:/# ceph orch ps
NAME                 HOST   STATUS        REFRESHED  AGE  VERSION  IMAGE NAME                  IMAGE ID      CONTAINER ID  
alertmanager.node1   node1  running (3m)  3m ago     4m   0.20.0   prom/alertmanager:v0.20.0   0881eb8f169f  b6284acbe6f3  
crash.node1          node1  running (4m)  3m ago     4m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  4fc0d53fd204  
grafana.node1        node1  running (3m)  3m ago     3m   6.6.2    ceph/ceph-grafana:6.6.2     a0dce381714a  5261bde49c13  
mgr.node1.dsbbba     node1  running (6m)  3m ago     6m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  0537b9df6bbd  
mon.node1            node1  running (6m)  3m ago     6m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  e749dae73c82  
node-exporter.node1  node1  running (3m)  3m ago     3m   0.18.1   prom/node-exporter:v0.18.1  e5a616e4b9cf  3c010514e9cc  
prometheus.node1     node1  running (3m)  3m ago     3m   2.18.1   prom/prometheus:v2.18.1     de242295e225  891b0844db44  
```
<br></br>



<h3 id=3>加入新的host</h3>

---
<br></br>
為了無密碼登入，將cephadm產生的public key複製進node2與node3
```
root@node1:/# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2
root@node1:/# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3
```
<br></br>

加入node2及node3
```
root@node1:/# ceph orch host add node2
Added host 'node2'
root@node1:/# ceph orch host add node3
Added host 'node3'
```
<br></br>

可以看到host增加了
```
root@node1:/# ceph orch host ls
HOST   ADDR   LABELS  STATUS  
node1  node1                  
node2  node2                  
node3  node3   
```
<br></br>

cephadm會自動在新增加的host上增加mon，並部屬相應的container
```
root@node2:/home/zaq# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
ea7ebd2112d7        prom/node-exporter:v0.18.1   "/bin/node_exporter …"   13 seconds ago      Up 12 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-node-exporter.node2
d18ece039c8b        ceph/ceph:v15                "/usr/bin/ceph-mon -…"   29 seconds ago      Up 29 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-mon.node2
d0ef6e23385a        ceph/ceph:v15                "/usr/bin/ceph-crash…"   33 seconds ago      Up 32 seconds                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-crash.node2
```
<br></br>

回到cephadm shell，可以看到cluster status中的mon增加了
```txt {hl_lines=[9]}
root@node1:/# ceph status
  cluster:
    id:     47f144d8-27bd-11eb-82c3-000c291ffa17
    health: HEALTH_WARN
            Reduced data availability: 1 pg inactive
            OSD count 0 < osd_pool_default_size 3
 
  services:
    mon: 3 daemons, quorum node1,node2,node3 (age 42s)
    mgr: node1.dsbbba(active, since 12m), standbys: node3.rqonis
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             1 unknown
```

<br></br>



<h3 id=4>加入新的osd</h3>

---
<br></br>
接下來部屬osd，可以用以下指令列出所有host上的device，只有乾淨的device可以部屬osd
```
root@node1:/# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS  
node1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                   
node1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked          
node2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                   
node2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked          
node3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                   
node3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked      
```
<br></br>

因為量少所以就apply all，如此一來所有空的device都會加入osd
```
root@node1:/# ceph orch apply osd --all-available-devices
```
<br></br>

會在host上部屬對應的container，每個osd都會有一個container
```txt {hl_lines=[3]}
root@node2:/home/zaq# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
8255092bd66c        ceph/ceph:v15                "/usr/bin/ceph-osd -…"   50 minutes ago      Up 50 minutes                           ceph-47f144d8-27bd-11eb-82c3-000c291ffa17-osd.1
...
```
<br></br>


回到node1上也能看到osd增加了
```txt {hl_lines=[9]}
root@node1:/# ceph status
  cluster:
    id:     47f144d8-27bd-11eb-82c3-000c291ffa17
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum node1,node2,node3 (age 5m)
    mgr: node1.dsbbba(active, since 17m), standbys: node3.rqonis
    osd: 3 osds: 3 up (since 86s), 3 in (since 86s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 57 GiB / 60 GiB avail
    pgs:     1 active+clean
```
<br></br>

部屬成功的話available會變成false
```
root@node1:/# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS                                          
node1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked                                                  
node1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  Insufficient space (<5GB) on vgs, LVM detected, locked  
node2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked                                                  
node2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  Insufficient space (<5GB) on vgs, locked, LVM detected  
node3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked                                                  
node3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  Insufficient space (<5GB) on vgs, locked, LVM detected  
```
<br></br>


如此一來就能獲得一個基礎的ceph cluster了