---
title: "使用Scrapy爬取PTT貼文"
date: 2020-09-07T09:54:48+08:00
draft: true
categories: [網路爬蟲]
tags: [Python, Scrapy]
---
前一篇練習比較陽春，沒有做到換頁的部分，
  
這次索性來做複雜一點，爬取媒體最愛的PTT，雖然本人不太逛PTT就是了…
<!--more-->
完成的souce code已放上github：[https://github.com/mikanbearer/scrapy_sample](https://github.com/mikanbearer/scrapy_sample)
  
一樣先把環境部屬好
```
$ scrapy startproject scrapy_sample
$ cd scrapy_sample/scrapy_sample/spiders
$ scrapy genspider ptt_gossiping www.ptt.cc/bbs/Gossiping
```
<br></br>
編輯第一個spider
```:scrapy_sample/spiders/ptt_gossiping.py {linenos=table, linenostart=1}
import scrapy


class PttGossipingSpider(scrapy.Spider):
    name = 'ptt_gossiping'
    allowed_domains = ['www.ptt.cc/bbs/Gossiping']
    start_urls = ['http://www.ptt.cc/bbs/Gossiping']

    def parse(self, response):
        print(response.text)
```
當然ptt的網站第一個會出現的就是確認年齡的頁面，之後要再改改
<br></br>
年齡確認用了一個form，會post兩種value，yes和no，就直接post了，省時省力
```:scrapy_sample/spiders/ptt_gossiping.py {linenos=table, linenostart=1, hl_lines=["9-19"]}
import scrapy


class PttGossipingSpider(scrapy.Spider):
    name = 'ptt_gossiping'
    allowed_domains = ['www.ptt.cc/bbs/Gossiping']
    start_urls = ['http://www.ptt.cc/bbs/Gossiping']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'yes': 'yes'},
            callback=self.after_ask,
            dont_filter=True
        )

    def after_ask(self, response):
        print(response.text)
```
<br></br>
再來就要開始換頁，把spider改成這樣，頁是最好先限制一下，不然好幾萬頁有點吃不消
```:scrapy_sample/spiders/ptt_gossiping.py {linenos=table, linenostart=1, hl_lines=[9, 10, "21-28"]}
import scrapy


class PttGossipingSpider(scrapy.Spider):
    name = 'ptt_gossiping'
    allowed_domains = ['www.ptt.cc/bbs/Gossiping']
    start_urls = ['http://www.ptt.cc/bbs/Gossiping']

    max_page = 10 #最大頁數
    page_count = 0 #計數用

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'yes': 'yes'},
            callback=self.after_ask,
            dont_filter=True
        )

    def after_ask(self, response):
        if self.page_count < self.max_page:
            next = response.xpath(
                '//div[@id="action-bar-container"]//a[contains(text(), "上頁")]/@href').get(default=None) #抓innertext為"上頁"的element
            if next:
                url = response.urljoin(next)
                print(url)
                self.page_count += 1
                yield scrapy.Request(url=url, callback=self.after_ask, dont_filter=True) #再call一次after_ask
```
<br></br>
接下來再小小修改一下after_ask()，使用一個for來多開幾個Reuqest，callback為parse_post
```:scrapy_sample/spiders/ptt_gossiping.py {linenos=table, linenostart=20, hl_lines=["2-4"]}
    def after_ask(self, response):
        for href in response.css('div.r-ent div.title a::attr(href)').getall():
            url = response.urljoin(href)
            yield scrapy.Request(url=url, callback=self.parse_post, dont_filter=True)

        if self.page_count < self.max_page:
            next = response.xpath(
                '//div[@id="action-bar-container"]//a[contains(text(), "上頁")]/@href').get(default=None)
            if next:
                self.page_count += 1
                url = response.urljoin(next)
                yield scrapy.Request(url=url, callback=self.after_ask, dont_filter=True)
```
<br></br>
先定義這個簡單的function，就能看到response.url是個標題的網址，如此一來就可以開始抓內容了
```py
    def parse_post(self, response):
        print(response.url)
```
<br></br>
準備好items
```:scrapy_sample/items.py {linenos=table, linenostart=1}
import scrapy


class PostItem(scrapy.Item):
    url = scrapy.Field()
    title = scrapy.Field()
    author = scrapy.Field()
    date = scrapy.Field()
    content = scrapy.Field()
    comments = scrapy.Field()
    score = scrapy.Field()

```
<br></br>
接下來回到spider，修改成下列的樣子就大功告成啦
```:scrapy_sample/spiders/ptt_gossiping.py {linenos=table, linenostart=35}
    def parse_post(self, response):
        item = PostItem()
        item['title'] = response.css('meta[property="og:title"]::attr(content)').get()
        item['author'] = response.xpath(
            '//div[@class="article-metaline"]/span[text()="作者"]/following-sibling::span[1]/text()').get().split(' ')[0]

        datetime_str = response.xpath(
            '//div[@class="article-metaline"]/span[text()="時間"]/following-sibling::span[1]/text()').get()
        item['date'] = datetime.strptime(datetime_str, '%a %b %d %H:%M:%S %Y')
        item['content'] = response.css('div#main-content::text').get()
        comments = []
        score_total = 0

        for comment in response.css('div.push'):
            tag = comment.css('span.push-tag::text').get()
            user = comment.css('span.push-userid::text').get()
            content = comment.css('span.push-content::text').get()

            if '推' in tag:
                score = 1
            elif '噓' in tag:
                score = -1
            else:
                score = 0

            score_total += score

            comments.append({
                'user': user,
                'score': score,
                'content': content
            })

        item['comments'] = comments
        item['score'] = score_total
        item['url'] = response.url

        yield item
```
<br></br>
完成輸入指令，就能看到貼文被抓下來啦
```
$ scrapy crawl ptt_gossiping -o test.json
```