



    
        
    




{
  "result": {
    "content": "\u003cp\u003ecephadm是官方推薦的安裝方式之一(另一個是rook)，先來試做看看順不順手，目標是建立一個可掛載使用的Cluster\u003c/p\u003e\n\u003cp\u003e準備三個guest os，這次環境是CentOS 7\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# cat /etc/*-release\r\nCentOS Linux release 7.7.1908 (Core)\r\nNAME=\u0026quot;CentOS Linux\u0026quot;\r\nVERSION=\u0026quot;7 (Core)\u0026quot;\r\n...\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e預計做成以下形式\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003enode1\u003c/td\u003e\n\u003ctd\u003e192.168.149.131\u003c/td\u003e\n\u003ctd\u003emgr, mon, osd, mds\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003enode2\u003c/td\u003e\n\u003ctd\u003e192.168.149.132\u003c/td\u003e\n\u003ctd\u003emon, osd, mds\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003enode3\u003c/td\u003e\n\u003ctd\u003e192.168.149.133\u003c/td\u003e\n\u003ctd\u003emon, osd, mds\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n主要步驟如下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca onclick=\"window.scrollTo({top: document.getElementById(1).offsetTop, behavior: 'smooth'})\"\u003e準備測試環境\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca onclick=\"window.scrollTo({top: document.getElementById(2).offsetTop, behavior: 'smooth'})\"\u003e建立Cluster\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca onclick=\"window.scrollTo({top: document.getElementById(3).offsetTop, behavior: 'smooth'})\"\u003e部屬OSD與MDS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca onclick=\"window.scrollTo({top: document.getElementById(4).offsetTop, behavior: 'smooth'})\"\u003e建立CephFS並掛載\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch3 id=1\u003e準備測試環境\u003c/h3\u003e\r\n\u003chr\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e首先安裝cephadm執行時需要的python3與校時的chrony\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# yum install -y python3 chrony\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n安裝完啟用chrony\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# systemctl start chronyd\r\n# systemctl enable chronyd\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\nCeph的daemon是以container形式存在於host上，所以需要docker或是podman，否則會出現這樣的錯誤\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eERROR: Unable to locate any of ['podman', 'docker']\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n這裡的範例是安裝docker(需增加repo)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# yum install -y yum-utils\r\n# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\r\n# yum install -y docker-ce docker-ce-cli containerd.io\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n安裝完記得啟動\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# systemctl start docker\r\n# systemctl enable docker\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e增加hosts\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF\r\n\u0026gt; 192.168.149.131 node1\r\n\u0026gt; 192.168.149.132 node2\r\n\u0026gt; 192.168.149.133 node3\r\n\u0026gt; EOF\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch3 id=2\u003e建立Cluster\u003c/h3\u003e\r\n\u003chr\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e以下動作全都在node1執行，接下來就是下載cephadm並增加執行權限，cephadm可以直接使用不需要安裝，所以這裡就不安裝了\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\r\n[root@node1 ~]# chmod +x cephadm \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e接下使用cephadm建立cluster，node1將作為mgr與mon\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# mkdir -p /etc/ceph\r\n[root@node1 ~]# ./cephadm bootstrap --mon-ip 192.168.149.131\r\n...\r\nCeph Dashboard is now available at:\r\n\r\n             URL: https://node1:8443/\r\n            User: admin\r\n        Password: krbpvjv6t3\r\n\r\nYou can access the Ceph CLI with:\r\n\r\n        sudo ./cephadm shell --fsid e36ef05a-287a-11eb-a943-000c2929c13d -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring\r\n\r\nPlease consider enabling telemetry to help improve Ceph:\r\n\r\n        ceph telemetry on\r\n\r\nFor more information see:\r\n\r\n        https://docs.ceph.com/docs/master/mgr/telemetry/\r\n\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e完成會有個不錯漂亮的grafana dashboard\n\u003cimg src=\"1.png\" alt=\"\"\u003e\n\u003cbr\u003e\u003c/br\u003e\n\u003cimg src=\"2.png\" alt=\"\"\u003e\n\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e可以看看cephadm部屬的container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# docker ps\r\nCONTAINER ID        IMAGE                       COMMAND                  CREATED              STATUS              PORTS               NAMES\r\n33e2256e001d        ceph/ceph:v15               \u0026quot;/usr/bin/ceph-crash…\u0026quot;   29 seconds ago       Up 28 seconds                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node1\r\n0e415c9753b5        prom/alertmanager:v0.20.0   \u0026quot;/bin/alertmanager -…\u0026quot;   33 seconds ago       Up 32 seconds                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-alertmanager.node1\r\ne1c850700429        ceph/ceph:v15               \u0026quot;/usr/bin/ceph-mgr -…\u0026quot;   About a minute ago   Up About a minute                       ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mgr.node1.vhujdv\r\n4a8881027cb6        ceph/ceph:v15               \u0026quot;/usr/bin/ceph-mon -…\u0026quot;   About a minute ago   Up About a minute                       ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node1\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e可以使用cephadm shell來使用ceph指令，並以此來操作cluster\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# ./cephadm shell\r\nInferring fsid e36ef05a-287a-11eb-a943-000c2929c13d\r\nInferring config /var/lib/ceph/e36ef05a-287a-11eb-a943-000c2929c13d/mon.node1/config\r\nUsing recent ceph image ceph/ceph:v15\r\n[ceph: root@node1 /]# \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e查看cluster狀態\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e[ceph: root@node1 /]# ceph status\n  cluster:\n    id:     e36ef05a-287a-11eb-a943-000c2929c13d\n    health: HEALTH_WARN\n            Reduced data availability: 1 pg inactive\n            OSD count 0 \u0026lt; osd_pool_default_size 3\n \n\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e  services:\n\u003c/span\u003e\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    mon: 1 daemons, quorum node1 (age 3m)\n\u003c/span\u003e\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    mgr: node1.vhujdv(active, since 2m)\n\u003c/span\u003e\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    osd: 0 osds: 0 up, 0 in\n\u003c/span\u003e \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   0 B used, 0 B / 0 B avail\n    pgs:     100.000% pgs unknown\n             1 unknown\n \n[ceph: root@node1 /]# ceph orch ps\nNAME                 HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                  IMAGE ID      CONTAINER ID  \nalertmanager.node1   node1  running (19s)  12s ago    2m   0.20.0   prom/alertmanager:v0.20.0   0881eb8f169f  3eda8ed9c0a4  \ncrash.node1          node1  running (2m)   12s ago    2m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  33e2256e001d  \ngrafana.node1        node1  running (16s)  12s ago    63s  6.6.2    ceph/ceph-grafana:6.6.2     a0dce381714a  bbda408ab66a  \nmgr.node1.vhujdv     node1  running (3m)   12s ago    3m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  e1c850700429  \nmon.node1            node1  running (3m)   12s ago    3m   15.2.5   docker.io/ceph/ceph:v15     4405f6339e35  4a8881027cb6  \nnode-exporter.node1  node1  running (49s)  12s ago    59s  0.18.1   prom/node-exporter:v0.18.1  e5a616e4b9cf  63daaba5a0a3  \nprometheus.node1     node1  running (33s)  12s ago    33s  2.18.1   prom/prometheus:v2.18.1     de242295e225  0c0ea49443a2   \n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n接下來加入新的Host，為了無密碼登入，將cephadm產生的public key複製進node2與node3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2\r\n[root@node1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e加入node2及node3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch host add node2\r\nAdded host 'node2'\r\n[ceph: root@node1 /]# ceph orch host add node3\r\nAdded host 'node3'\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e可以看到host增加了\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch host ls       \r\nHOST   ADDR   LABELS  STATUS  \r\nnode1  node1                  \r\nnode2  node2                  \r\nnode3  node3    \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003ecephadm會自動在新增加的host上增加mon，並部屬相應的container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node2 ~]# docker ps\r\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES\r\n10ff38cdf622        prom/node-exporter:v0.18.1   \u0026quot;/bin/node_exporter …\u0026quot;   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-node-exporter.node2\r\n1207bf201a7d        ceph/ceph:v15                \u0026quot;/usr/bin/ceph-mon -…\u0026quot;   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node2\r\n3fa73ca5bf9c        ceph/ceph:v15                \u0026quot;/usr/bin/ceph-crash…\u0026quot;   15 minutes ago      Up 15 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node2\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e回到cephadm shell，可以看到cluster status中的mon增加了，且會自動產生一個standby的mgr\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e[ceph: root@node1 /]# ceph status\n  cluster:\n    id:     e36ef05a-287a-11eb-a943-000c2929c13d\n    health: HEALTH_WARN\n            Reduced data availability: 1 pg inactive\n            OSD count 0 \u0026lt; osd_pool_default_size 3\n \n  services:\n\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    mon: 3 daemons, quorum node1,node2,node3 (age 17m)\n\u003c/span\u003e    mgr: node1.vhujdv(active, since 46m), standbys: node3.djmubq\n    osd: 0 osds: 0 up, 0 in\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   0 B used, 0 B / 0 B avail\n    pgs:     100.000% pgs unknown\n             1 unknown\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch3 id=3\u003e部屬OSD與MDS\u003c/h3\u003e\r\n\u003chr\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n接下來部屬osd，可以用以下指令列出所有host上的device，只有乾淨的device可以部屬osd\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch device ls\r\nHOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS                                          \r\nnode1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           \r\nnode1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (\u0026lt;5GB) on vgs, LVM detected  \r\nnode2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           \r\nnode2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, locked, Insufficient space (\u0026lt;5GB) on vgs  \r\nnode3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           True                                                           \r\nnode3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, LVM detected, Insufficient space (\u0026lt;5GB) on vgs     \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n可用以下指令部屬osd\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# ceph orch daemon add osd *\u0026lt;host\u0026gt;*:*\u0026lt;device-path\u0026gt;*\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e但為了方便就apply all，如此一來就會為所有空的device部屬osd\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch apply osd --all-available-devices\r\nScheduled osd.all-available-devices update...\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e會在host上部屬對應的container，每個osd都會有一個container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e[root@node2 ~]# docker ps\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES\n5fb62081081a        ceph/ceph:v15                \u0026#34;/usr/sbin/ceph-volu…\u0026#34;   1 second ago        Up 1 second                             gallant_hellman\n\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003edc8997dca2e2        ceph/ceph:v15                \u0026#34;/usr/bin/ceph-osd -…\u0026#34;   8 seconds ago       Up 7 seconds                            ceph-e36ef05a-287a-11eb-a943-000c2929c13d-osd.0\n\u003c/span\u003e10ff38cdf622        prom/node-exporter:v0.18.1   \u0026#34;/bin/node_exporter …\u0026#34;   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-node-exporter.node2\n1207bf201a7d        ceph/ceph:v15                \u0026#34;/usr/bin/ceph-mon -…\u0026#34;   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-mon.node2\n3fa73ca5bf9c        ceph/ceph:v15                \u0026#34;/usr/bin/ceph-crash…\u0026#34;   19 minutes ago      Up 19 minutes                           ceph-e36ef05a-287a-11eb-a943-000c2929c13d-crash.node2\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e回到node1上也能看到osd增加了\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e[ceph: root@node1 /]# ceph status\n  cluster:\n    id:     e36ef05a-287a-11eb-a943-000c2929c13d\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3 (age 19m)\n    mgr: node1.vhujdv(active, since 48m), standbys: node3.djmubq\n\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    osd: 3 osds: 3 up (since 33s), 3 in (since 33s)\n\u003c/span\u003e \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   3.0 GiB used, 57 GiB / 60 GiB avail\n    pgs:     1 active+clean\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e部屬成功的話available會變成false\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch device ls\r\nHOST   PATH      TYPE   SIZE  DEVICE_ID  MODEL             VENDOR   ROTATIONAL  AVAIL  REJECT REASONS                                          \r\nnode1  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (\u0026lt;5GB) on vgs, locked  \r\nnode1  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (\u0026lt;5GB) on vgs, locked  \r\nnode2  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (\u0026lt;5GB) on vgs, locked  \r\nnode2  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  LVM detected, Insufficient space (\u0026lt;5GB) on vgs, locked  \r\nnode3  /dev/sda  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (\u0026lt;5GB) on vgs, LVM detected  \r\nnode3  /dev/sdb  hdd   20.0G             VMware Virtual S  VMware,  1           False  locked, Insufficient space (\u0026lt;5GB) on vgs, LVM detected  \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n可透過以下指令查看osd的狀態\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph osd tree\r\nID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF\r\n-1         0.05846  root default                             \r\n-7         0.01949      host node1                           \r\n 2    hdd  0.01949          osd.2       up   1.00000  1.00000\r\n-3         0.01949      host node2                           \r\n 0    hdd  0.01949          osd.0       up   1.00000  1.00000\r\n-5         0.01949      host node3                           \r\n 1    hdd  0.01949          osd.1       up   1.00000  1.00000\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003cp\u003e接下來部屬mds使用label來部屬，首先三台node都加上mds的label\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch host label add node1 mds\r\nAdded label mds to host node1\r\n[ceph: root@node1 /]# ceph orch host label add node2 mds\r\nAdded label mds to host node2\r\n[ceph: root@node1 /]# ceph orch host label add node3 mds\r\nAdded label mds to host node3\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\nhost的label會變成這種狀態\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch host ls\r\nHOST   ADDR   LABELS  STATUS  \r\nnode1  node1  mds             \r\nnode2  node2  mds             \r\nnode3  node3  mds  \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\napply給他打下去\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph orch apply mds cephfs label:mds\r\nScheduled mds.cephfs update...\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n如此一來會多三個standby的mds\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e[ceph: root@node1 /]# ceph status\n  cluster:\n    id:     e36ef05a-287a-11eb-a943-000c2929c13d\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3 (age 77m)\n    mgr: node1.vhujdv(active, since 106m), standbys: node3.djmubq\n\u003cspan style=\"display:block;width:100%;background-color:#3c3d38\"\u003e    mds:  3 up:standby\n\u003c/span\u003e    osd: 3 osds: 3 up (since 58m), 3 in (since 58m)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   3.0 GiB used, 57 GiB / 60 GiB avail\n    pgs:     1 active+clean\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n也能用以下指令看狀態\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph fs status\r\n    STANDBY MDS      \r\ncephfs.node3.vpooup   \r\ncephfs.node2.dakinl  \r\ncephfs.node1.rvifdz\r\nMDS version: ceph version 15.2.5 (2c93eff00150f0cc5f106a559557a58d3d7b6f1f) octopus (stable)\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\u003c/p\u003e\n\u003ch3 id=4\u003e建立CephFS並掛載\u003c/h3\u003e\r\n\u003chr\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n接下來就讓Ceph自動建立一個名為cephfs的CephFS\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph fs volume create cephfs\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n建立完成後就變成一個active，兩個standby，做到這種狀態就能掛載使用了\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[ceph: root@node1 /]# ceph fs status\r\ncephfs - 0 clients\r\n======\r\nRANK  STATE           MDS             ACTIVITY     DNS    INOS  \r\n 0    active  cephfs.node1.rvifdz  Reqs:    0 /s    10     13   \r\n       POOL           TYPE     USED  AVAIL  \r\ncephfs.cephfs.meta  metadata  1536k  17.9G  \r\ncephfs.cephfs.data    data       0   17.9G  \r\n    STANDBY MDS      \r\ncephfs.node2.dakinl  \r\ncephfs.node3.vpooup  \r\nMDS version: ceph version 15.2.5 (2c93eff00150f0cc5f106a559557a58d3d7b6f1f) octopus (stable)\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\n最後就是掛載了，client需要的key在cephadm的host內找\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring \r\n[client.admin]\r\n        key = AQDUMrNf+GymGhAAcqXCfoQjf1A+KXq+dqIPLQ==\r\n\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cbr\u003e\u003c/br\u003e\nmount指令如下，ip位置為monitor的ip\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[root@node2 ~]# mkdir -p /mnt/ceph\r\n[root@node2 ~]# mount -t ceph 192.168.149.131:/ /mnt/cephfs -o name=admin,secret=AQDUMrNf+GymGhAAcqXCfoQjf1A+KXq+dqIPLQ==\r\n\u003c/code\u003e\u003c/pre\u003e",
    "kind": "page",
    "params": {
      "categories": [
        "Deployment"
      ],
      "date": "2020-11-10T16:52:54+08:00",
      "draft": false,
      "iscjklanguage": true,
      "lastmod": "2020-11-10T16:52:54+08:00",
      "publishdate": "2020-11-10T16:52:54+08:00",
      "tags": [
        "Ceph"
      ],
      "title": "使用cephadm部屬Ceph Cluster"
    },
    "permalink": "https://mikanbearer.github.io/posts/2020/11/202011101652/index.json",
    "type": "posts",
    "wordcount": 2504
  }
}