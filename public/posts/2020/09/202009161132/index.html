<p>身為資料分析&amp;機器學習的超級新手，個人還是覺得實際操作一遍比理論更好理解，
這次練習的是之前曾草草帶過的機械學習工具scikit-learn，主題就決定是決策樹了</p>
<p>首先要提一下決策樹，決策樹顧名思義是一種樹形結構，利用層層推理來進行數據的分類，決策樹的構成可分為：</p>
<ul>
<li>根節點(root node)： 全部的數據樣本</li>
<li>內部節點(internal node)： 對應的特徵屬性測試</li>
<li>葉節點(leaf node)： 決策的結果</li>
</ul>
<p><br></br>
以下圖範例來看的話，A就是根節點，C、D、E則是葉節點，剩餘的B就是內部節點</p>
<p><img src="3.png" alt=""></p>
<p>本次練習的參考資料：</p>
<ul>
<li><a href="https://qiita.com/3000manJPY/items/ef7495960f472ec14377">https://qiita.com/3000manJPY/items/ef7495960f472ec14377</a></li>
<li><a href="https://qiita.com/Hawaii/items/53efe3e96b1171ebc7db">https://qiita.com/Hawaii/items/53efe3e96b1171ebc7db</a></li>
</ul>
<p><br></br>
做好的結果也可直接觀看<a href="https://github.com/mikanbearer/experiments/blob/master/tree.ipynb">https://github.com/mikanbearer/experiments/blob/master/tree.ipynb</a></p>
<p><br></br></p>
<h3 id="載入測試用的資料集">載入測試用的資料集</h3>
<hr>
<p><br></br>
scikit-learn提供了很多練習用的資料集方便練習，這次練習使用load_wine()，可以從DESCR查看詳細資料</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> datasets


wine <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>load_wine()

<span style="color:#66d9ef">print</span>(wine<span style="color:#f92672">.</span>keys())
<span style="color:#75715e">#dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;])</span>

<span style="color:#66d9ef">print</span>(wine<span style="color:#f92672">.</span>DESCR)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">.. _wine_dataset:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Wine recognition dataset
</span><span style="color:#e6db74">------------------------
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">**Data Set Characteristics:**
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    :Number of Instances: 178 (50 in each of three classes)
</span><span style="color:#e6db74">    :Number of Attributes: 13 numeric, predictive attributes and the class
</span><span style="color:#e6db74">    :Attribute Information:
</span><span style="color:#e6db74"> 		- Alcohol
</span><span style="color:#e6db74"> 		- Malic acid
</span><span style="color:#e6db74"> 		- Ash
</span><span style="color:#e6db74">		- Alcalinity of ash  
</span><span style="color:#e6db74"> 		- Magnesium
</span><span style="color:#e6db74">		- Total phenols
</span><span style="color:#e6db74"> 		- Flavanoids
</span><span style="color:#e6db74"> 		- Nonflavanoid phenols
</span><span style="color:#e6db74"> 		- Proanthocyanins
</span><span style="color:#e6db74">		- Color intensity
</span><span style="color:#e6db74"> 		- Hue
</span><span style="color:#e6db74"> 		- OD280/OD315 of diluted wines
</span><span style="color:#e6db74"> 		- Proline
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    - class:
</span><span style="color:#e6db74">            - class_0
</span><span style="color:#e6db74">            - class_1
</span><span style="color:#e6db74">            - class_2
</span><span style="color:#e6db74">		
</span><span style="color:#e6db74">    :Summary Statistics:
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    ============================= ==== ===== ======= =====
</span><span style="color:#e6db74">                                   Min   Max   Mean     SD
</span><span style="color:#e6db74">    ============================= ==== ===== ======= =====
</span><span style="color:#e6db74">    Alcohol:                      11.0  14.8    13.0   0.8
</span><span style="color:#e6db74">    Malic Acid:                   0.74  5.80    2.34  1.12
</span><span style="color:#e6db74">    Ash:                          1.36  3.23    2.36  0.27
</span><span style="color:#e6db74">    Alcalinity of Ash:            10.6  30.0    19.5   3.3
</span><span style="color:#e6db74">    Magnesium:                    70.0 162.0    99.7  14.3
</span><span style="color:#e6db74">    Total Phenols:                0.98  3.88    2.29  0.63
</span><span style="color:#e6db74">    Flavanoids:                   0.34  5.08    2.03  1.00
</span><span style="color:#e6db74">    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12
</span><span style="color:#e6db74">    Proanthocyanins:              0.41  3.58    1.59  0.57
</span><span style="color:#e6db74">    Colour Intensity:              1.3  13.0     5.1   2.3
</span><span style="color:#e6db74">    Hue:                          0.48  1.71    0.96  0.23
</span><span style="color:#e6db74">    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71
</span><span style="color:#e6db74">    Proline:                       278  1680     746   315
</span><span style="color:#e6db74">    ============================= ==== ===== ======= =====
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    :Missing Attribute Values: None
</span><span style="color:#e6db74">    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)
</span><span style="color:#e6db74">    :Creator: R.A. Fisher
</span><span style="color:#e6db74">    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
</span><span style="color:#e6db74">    :Date: July, 1988
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">This is a copy of UCI ML Wine recognition datasets.
</span><span style="color:#e6db74">https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">The data is the results of a chemical analysis of wines grown in the same
</span><span style="color:#e6db74">region in Italy by three different cultivators. There are thirteen different
</span><span style="color:#e6db74">measurements taken for different constituents found in the three types of
</span><span style="color:#e6db74">wine.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Original Owners: 
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Forina, M. et al, PARVUS - 
</span><span style="color:#e6db74">An Extendible Package for Data Exploration, Classification and Correlation. 
</span><span style="color:#e6db74">Institute of Pharmaceutical and Food Analysis and Technologies,
</span><span style="color:#e6db74">Via Brigata Salerno, 16147 Genoa, Italy.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Citation:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Lichman, M. (2013). UCI Machine Learning Repository
</span><span style="color:#e6db74">[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
</span><span style="color:#e6db74">School of Information and Computer Science. 
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">.. topic:: References
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">  (1) S. Aeberhard, D. Coomans and O. de Vel, 
</span><span style="color:#e6db74">  Comparison of Classifiers in High Dimensional Settings, 
</span><span style="color:#e6db74">  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  
</span><span style="color:#e6db74">  Mathematics and Statistics, James Cook University of North Queensland. 
</span><span style="color:#e6db74">  (Also submitted to Technometrics). 
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">  The data was used with many others for comparing various 
</span><span style="color:#e6db74">  classifiers. The classes are separable, though only RDA 
</span><span style="color:#e6db74">  has achieved 100</span><span style="color:#e6db74">% c</span><span style="color:#e6db74">orrect classification. 
</span><span style="color:#e6db74">  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) 
</span><span style="color:#e6db74">  (All results using the leave-one-out technique) 
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">  (2) S. Aeberhard, D. Coomans and O. de Vel, 
</span><span style="color:#e6db74">  &#34;THE CLASSIFICATION PERFORMANCE OF RDA&#34; 
</span><span style="color:#e6db74">  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of 
</span><span style="color:#e6db74">  Mathematics and Statistics, James Cook University of North Queensland. 
</span><span style="color:#e6db74">  (Also submitted to Journal of Chemometrics).
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p><br></br>
為了方便練習，用pandas做成DataFrame</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd


wine_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(data<span style="color:#f92672">=</span>wine<span style="color:#f92672">.</span>data, columns<span style="color:#f92672">=</span>wine<span style="color:#f92672">.</span>feature_names)
<span style="color:#66d9ef">print</span>(wine_data<span style="color:#f92672">.</span>head())
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">   alcohol  malic_acid   ash  ...   hue  od280/od315_of_diluted_wines  proline
</span><span style="color:#e6db74">0    14.23        1.71  2.43  ...  1.04                          3.92   1065.0
</span><span style="color:#e6db74">1    13.20        1.78  2.14  ...  1.05                          3.40   1050.0
</span><span style="color:#e6db74">2    13.16        2.36  2.67  ...  1.03                          3.17   1185.0
</span><span style="color:#e6db74">3    14.37        1.95  2.50  ...  0.86                          3.45   1480.0
</span><span style="color:#e6db74">4    13.24        2.59  2.87  ...  1.04                          2.93    735.0
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[5 rows x 13 columns]
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>

wine_target <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(data<span style="color:#f92672">=</span>wine<span style="color:#f92672">.</span>target)
<span style="color:#66d9ef">print</span>(wine_target<span style="color:#f92672">.</span>head())
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">   0
</span><span style="color:#e6db74">0  0
</span><span style="color:#e6db74">1  0
</span><span style="color:#e6db74">2  0
</span><span style="color:#e6db74">3  0
</span><span style="color:#e6db74">4  0
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p><br></br></p>
<h3 id="分割訓練集與測試集">分割訓練集與測試集</h3>
<hr>
<p><br></br>
在這裡利用train_test_split分割出&quot;訓練集(training set)&ldquo;與&quot;測試集(test set)&quot;，訓練集是模型擬合用的樣本資料，而測試集則是預留下來對結果進行評價</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split


x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(wine_data, wine_target, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>)

<span style="color:#66d9ef">print</span>(wine_data<span style="color:#f92672">.</span>shape)
<span style="color:#75715e">#(178, 13)</span>
<span style="color:#66d9ef">print</span>(x_train<span style="color:#f92672">.</span>shape)
<span style="color:#75715e">#(124, 13)</span>
<span style="color:#66d9ef">print</span>(x_test<span style="color:#f92672">.</span>shape)
<span style="color:#75715e">#(54, 13)</span>
</code></pre></div><p><br></br></p>
<h3 id="實施訓練">實施訓練</h3>
<hr>
<p><br></br>
這個DecisionTreeClassifier就是咱們的分類器，要使用剛才的資料來訓練模型，就把剛才分出來的訓練集往分類器裡面丟，當然也可以是資料的狀況設定max_depth或max_leaf_nodes，這樣決策樹就完成了，能使用plot_tree()來看看訓練的結果</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier, plot_tree


clf <span style="color:#f92672">=</span> DecisionTreeClassifier()
clf<span style="color:#f92672">.</span>fit(x_train, y_train)

plot_tree(clf)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">[Text(136.96363636363637, 195.696, &#39;X[9] &lt;= 3.82</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.657</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 124</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [39, 51, 34]&#39;),
</span><span style="color:#e6db74"> Text(60.872727272727275, 152.208, &#39;X[11] &lt;= 3.73</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.087</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 44</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [2, 42, 0]&#39;),
</span><span style="color:#e6db74"> Text(30.436363636363637, 108.72, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 42</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [0, 42, 0]&#39;),
</span><span style="color:#e6db74"> Text(91.30909090909091, 108.72, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 2</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [2, 0, 0]&#39;),
</span><span style="color:#e6db74"> Text(213.05454545454546, 152.208, &#39;X[11] &lt;= 2.475</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.593</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 80</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [37, 9, 34]&#39;),
</span><span style="color:#e6db74"> Text(152.1818181818182, 108.72, &#39;X[10] &lt;= 0.97</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.056</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 35</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [0, 1, 34]&#39;),
</span><span style="color:#e6db74"> Text(121.74545454545455, 65.232, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 34</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [0, 0, 34]&#39;),
</span><span style="color:#e6db74"> Text(182.61818181818182, 65.232, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 1</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [0, 1, 0]&#39;),
</span><span style="color:#e6db74"> Text(273.92727272727274, 108.72, &#39;X[12] &lt;= 724.5</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.292</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 45</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [37, 8, 0]&#39;),
</span><span style="color:#e6db74"> Text(243.4909090909091, 65.232, &#39;X[1] &lt;= 3.015</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">gini = 0.198</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 9</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [1, 8, 0]&#39;),
</span><span style="color:#e6db74"> Text(213.05454545454546, 21.744, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 8</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [0, 8, 0]&#39;),
</span><span style="color:#e6db74"> Text(273.92727272727274, 21.744, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 1</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [1, 0, 0]&#39;),
</span><span style="color:#e6db74"> Text(304.3636363636364, 65.232, &#39;gini = 0.0</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">samples = 36</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">value = [36, 0, 0]&#39;)]
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p><img src="0.png" alt="">
<br></br></p>
<h3 id="繪製決策樹">繪製決策樹</h3>
<hr>
<p><br></br>
前面的小樹實在慘不忍睹，這裡就用graphviz來將決策樹繪製出來</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> pydotplus
<span style="color:#f92672">from</span> IPython.display <span style="color:#f92672">import</span> Image
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> export_graphviz


dot_data <span style="color:#f92672">=</span> export_graphviz(
    clf,
    out_file<span style="color:#f92672">=</span>None,
    feature_names<span style="color:#f92672">=</span>wine<span style="color:#f92672">.</span>feature_names,
    class_names<span style="color:#f92672">=</span>wine<span style="color:#f92672">.</span>target_names,
    filled<span style="color:#f92672">=</span>True,
    proportion<span style="color:#f92672">=</span>True)
graph <span style="color:#f92672">=</span> pydotplus<span style="color:#f92672">.</span>graph_from_dot_data(dot_data)
Image(graph<span style="color:#f92672">.</span>create_png())
</code></pre></div><p><img src="1.png" alt="">
<br></br></p>
<h3 id="評價">評價</h3>
<hr>
<p><br></br>
經過決策樹分歧產生的預測和測試集中的實際資料對比</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix


df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(confusion_matrix(y_test,clf<span style="color:#f92672">.</span>predict(x_test)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)))
df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>rename(columns<span style="color:#f92672">=</span>dict(zip(df<span style="color:#f92672">.</span>columns, [<span style="color:#e6db74">&#39;(pre) &#39;</span> <span style="color:#f92672">+</span> name <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> wine<span style="color:#f92672">.</span>target_names])), index<span style="color:#f92672">=</span>dict(zip(df<span style="color:#f92672">.</span>columns, wine<span style="color:#f92672">.</span>target_names)))
df
</code></pre></div><p><img src="2.png" alt="">
<br></br>
算出評分，這次勉強還行，因為神經網路的隨機性，有時還會只有0.8左右</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">clf<span style="color:#f92672">.</span>score(x_test,y_test)

<span style="color:#75715e">#0.9444444444444444</span>
</code></pre></div>